{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ff260a-e226-45ab-830c-8152149f3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем необходимые библиотеки\n",
    "import cv2\n",
    "import mediapipe as mp #face detector\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06982039-7586-4bd5-bebe-e17cf44349e5",
   "metadata": {},
   "source": [
    "#### Sub functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8b869f-7c10-4b12-96fc-94e8b35953b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для предобработки изображений перед передачей их на модель торча\n",
    "def pth_processing(fp):\n",
    "    #класс используется для выполнения предварительной обработки входного массива данных (тензора)\n",
    "    class PreprocessInput(torch.nn.Module):\n",
    "        def init(self):\n",
    "            #вызывается инициализация родительского класса с помощью `super()`, чтобы убедиться, что базовые настройки корректны\n",
    "            super(PreprocessInput, self).init()\n",
    "\n",
    "        #метод определяет, как данные будут обрабатываться, когда они проходят через модуль\n",
    "        def forward(self, x):\n",
    "            #преобразуем входной тензор `x` в тип данных `torch.float32`\n",
    "            x = x.to(torch.float32)\n",
    "            #обращаем порядок всех каналов (например, если это изображение с тремя цветными каналами RGB, то порядок будет изменен с RGB на BGR)\n",
    "            x = torch.flip(x, dims=(0,))\n",
    "            #проводим нормализацию каналов RGB, вычитая специфические значения средних значений для каждого канала для того, \n",
    "            #чтобы улучшить качество входных данных перед подачей в модель\n",
    "            x[0, :, :] -= 91.4953\n",
    "            x[1, :, :] -= 103.8827\n",
    "            x[2, :, :] -= 131.0912\n",
    "            return x\n",
    "\n",
    "    #обработки изображения, чтобы подготовить его для модели\n",
    "    def get_img_torch(img):\n",
    "        \n",
    "        #создаем последовательность преобразований (pipeline), которая сначала преобразует изображение в тензор, \n",
    "        #а затем применяет к нему модуль `PreprocessInput`\n",
    "        ttransform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            PreprocessInput()\n",
    "        ])\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        img = ttransform(img)\n",
    "        #добавляем новую ось к тензору, чтобы подготовить его к обработке моделью \n",
    "        #(т.е. делаем его батчем с размерностью `(1, C, H, W)`) и перемещаем его на CPU.\n",
    "        img = torch.unsqueeze(img, 0).to('cpu')\n",
    "        return img\n",
    "    return get_img_torch(fp)\n",
    "\n",
    "#преобразовываем нормализованные координаты в пиксельные\n",
    "def norm_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    \n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    \n",
    "    return x_px, y_px\n",
    "\n",
    "#функция нужна для вычисления ограничивающего прямоугольника (bounding box) на основе заданных координат ключевых точек (landmarks)\n",
    "def get_box(fl, w, h):\n",
    "    #пустой словарь `idx_to_coors`, который будет использоваться для сопоставления индексов ключевых точек с их пиксельными координатами\n",
    "    idx_to_coors = {}\n",
    "    #цикл по ключевым точкам\n",
    "    for idx, landmark in enumerate(fl.landmark):\n",
    "        landmark_px = norm_coordinates(landmark.x, landmark.y, w, h)\n",
    "\n",
    "        if landmark_px:\n",
    "            idx_to_coors[idx] = landmark_px\n",
    "\n",
    "    #вычисление ограничивающего прямоугольника\n",
    "    x_min = np.min(np.asarray(list(idx_to_coors.values()))[:,0])\n",
    "    y_min = np.min(np.asarray(list(idx_to_coors.values()))[:,1])\n",
    "    endX = np.max(np.asarray(list(idx_to_coors.values()))[:,0])\n",
    "    endY = np.max(np.asarray(list(idx_to_coors.values()))[:,1])\n",
    "\n",
    "    #ограничение координат\n",
    "    (startX, startY) = (max(0, x_min), max(0, y_min))\n",
    "    (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "    \n",
    "    return startX, startY, endX, endY\n",
    "\n",
    "#используется для отображения результатов предсказаний на изображении\n",
    "def display_EMO_PRED(img, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255), line_width=2, ):\n",
    "    #задание параметров для рисования\n",
    "    lw = line_width or max(round(sum(img.shape) / 2 * 0.003), 2)\n",
    "    text2_color = (255, 0, 255)\n",
    "    #определение точек для прямоугольника\n",
    "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    #рисование ограничивающего прямоугольника\n",
    "    cv2.rectangle(img, p1, p2, text2_color, thickness=lw, lineType=cv2.LINE_AA)\n",
    "    #определение параметров текста\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    tf = max(lw - 1, 1)\n",
    "    text_fond = (0, 0, 0)\n",
    "    text_width_2, text_height_2 = cv2.getTextSize(label, font, lw / 3, tf)\n",
    "    text_width_2 = text_width_2[0] + round(((p2[0] - p1[0]) * 10) / 360)\n",
    "    center_face = p1[0] + round((p2[0] - p1[0]) / 2)\n",
    "\n",
    "    #добавление текста на изображение\n",
    "    cv2.putText(img, label,\n",
    "                (center_face - round(text_width_2 / 2), p1[1] - round(((p2[0] - p1[0]) * 20) / 360)), font,\n",
    "                lw / 3, text2_color, thickness=tf, lineType=cv2.LINE_AA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5eb65-7e6b-4919-8354-c0594f1ddd8f",
   "metadata": {},
   "source": [
    "#### Testing models by webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fffbfbd-5f4e-4fa0-8c67-4a8e71c47f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AISD\\OurProject\\OurProject\\project_av.emotion\\Recognize_emotions\\env\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "#инициализируем объекты mediapipe для рисования лицевой сетки и контуров\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "#создаем объект, определяющий стиль рисования для контуров лица\n",
    "my_drawing_specs = mp_drawing.DrawingSpec(color = (0, 255, 0), thickness = 1)\n",
    "\n",
    "#задаем переменную, которая хранит имя модели для удобства\n",
    "name = '0_66_49_wo_gl'\n",
    "\n",
    "#с помощью торча методом компиляции в моменте загружаем обученную модель на CPU (вместо 0 вставится name)\n",
    "pth_model = torch.jit.load('models_EmoAffectnet/torchscript_model_{0}.pth'.format(name)).to('cpu')\n",
    "#переключение модели в режим валидации (оценки)\n",
    "pth_model.eval()\n",
    "\n",
    "#создаем словарь, в котором определяем перечень эмоций\n",
    "DICT_EMO = {0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger'}\n",
    "\n",
    "#открываем первую доступную камеру\n",
    "cap = cv2.VideoCapture(0)\n",
    "#получаем ширину и высоту кадра\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "#инциализируем модуль Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "#параметры для стабилизации эмоций\n",
    "emotion_history = []  #история предсказанных эмоций для усреднения и предотвращения резких смен эмоций\n",
    "max_history_length = 10  #сколько последних предсказаний сохранять для сглаживания\n",
    "dominant_emotion = \"Neutral\"  #текущая доминирующая эмоция\n",
    "previous_emotion = \"Neutral\"  #предыдущая эмоция для плавной смены\n",
    "#количество кадров с новой эмоцией для подтверждения смены эмоции\n",
    "emotion_threshold = 5\n",
    "#минимальный уровень уверенности для принятия эмоции\n",
    "confidence_threshold = 0.9  #минимальная уверенность (от 0 до 1)\n",
    "#переменные для отслеживания стабильности эмоций\n",
    "emotion_change_counter = 0  #счетчик изменения эмоции\n",
    "current_emotion_streak = 0  #количество кадров с текущей новой эмоцией\n",
    "last_detected_emotion = \"Neutral\"  #последняя обнаруженная эмоция\n",
    "  \n",
    "with mp_face_mesh.FaceMesh(\n",
    "max_num_faces=1, #количество одновременно распознанных лиц на изображении\n",
    "refine_landmarks=True, #уточненные ключевые точки лица для детализированного результата\n",
    "min_detection_confidence=0.5,  #порог уверенности в том, что найденный объект является лицом\n",
    "min_tracking_confidence=0.5) as face_mesh: #минимальный уровень уверенности, необходимый для отслеживания лица между кадрами\n",
    "\n",
    "    #основной цикл обработки кадров\n",
    "    while cap.isOpened():\n",
    "        #захватываем кадр с камеры; возвращаются успешность захвата и кадр в виде массива пикселей\n",
    "        success, img = cap.read()\n",
    "        #если возникли проблемы с захватом изображения с камеры, закрываем окно\n",
    "        if not success: break\n",
    "\n",
    "        #обработка захваченного кадра с помощью модели распознавания лицевой сетки\n",
    "        results = face_mesh.process(img)\n",
    "\n",
    "        #если на изображении распознаны лица\n",
    "        if results.multi_face_landmarks:\n",
    "            #то для каждого лица\n",
    "            for fl in results.multi_face_landmarks:\n",
    "\n",
    "                #рисуется лицевая сетка с использованием стиля FACEMESH_TESSELATION\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    #изображение, на которое будут наноситься ключевые точки и их соединения\n",
    "                    image = img,\n",
    "                    #ключевые точки, содержащие координаты различных частей лица\n",
    "                    landmark_list = fl,\n",
    "                    #соединеняем ключевые точки лица в треугольники\n",
    "                    connections = mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    #ключевые точки рисоваться не будут\n",
    "                    landmark_drawing_spec = None,\n",
    "                    #используем предустановленный стиль для рисования лицевой сетки\n",
    "                    connection_drawing_spec = mp_drawing_styles\n",
    "                    .get_default_face_mesh_tesselation_style()\n",
    "                )\n",
    "                #рисуются контуры лица с использованием стиля рисования my_drawing_specs\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=img,\n",
    "                    landmark_list=fl,\n",
    "                    #какие точки нужно соединить для рисования контуров (по краю лица, глаз, губ и т.д.)\n",
    "                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    #используем кастомный стиль для рисования контуров лица\n",
    "                    connection_drawing_spec = my_drawing_specs\n",
    "                    #.get_default_face_mesh_tesselation_style()\n",
    "                )\n",
    "\n",
    "                #инициализация начальных координат\n",
    "                h, w, _ = img.shape\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "\n",
    "                #для каждой ключевой точки лица вычисляем координаты, определяющие область лица\n",
    "                for landmark in fl.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "\n",
    "                #устанавливаем значение отступа (в пикселях), которое будет добавлено к координатам ограничивающего прямоугольника\n",
    "                padding = 20\n",
    "                #корректировка координат с добавлением отступа\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(w, x_max + padding)\n",
    "                y_max = min(h, y_max + padding)\n",
    "\n",
    "                #получение ограничивающего прямоугольника для лица\n",
    "                startX, startY, endX, endY  = get_box(fl, w, h)\n",
    "                #извлечение области лица из изображения\n",
    "                cur_face = img[startY:endY, startX: endX]\n",
    "\n",
    "                #предобработка извлеченной области\n",
    "                cur_face = pth_processing(Image.fromarray(cur_face))\n",
    "                #предсказание эмоции с использованием модели\n",
    "                output = torch.nn.functional.softmax(pth_model(cur_face), dim=1).cpu().detach().numpy()\n",
    "                \n",
    "                #получение класса и метки эмоции\n",
    "                cl = np.argmax(output)\n",
    "                label = DICT_EMO[cl]\n",
    "\n",
    "                detected_emotion = label\n",
    "                #если новая эмоция не совпадает с предыдущей обнаруженной\n",
    "                if detected_emotion != last_detected_emotion:\n",
    "                    current_emotion_streak = 0  #сбрасываем счетчик стабильности\n",
    "                    last_detected_emotion = detected_emotion\n",
    "                else:\n",
    "                    current_emotion_streak += 1  #увеличиваем счетчик\n",
    "\n",
    "                #если эмоция повторяется достаточное количество кадров, она становится доминирующей\n",
    "                if current_emotion_streak >= emotion_threshold:\n",
    "                    dominant_emotion = detected_emotion\n",
    "                    current_emotion_streak = 0\n",
    "\n",
    "                #плавное изменение цвета текста эмоции для эффекта плавной смены\n",
    "                if dominant_emotion != previous_emotion:\n",
    "                    cv2.putText(img, dominant_emotion, (x_min, y_min - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    previous_emotion = dominant_emotion\n",
    "                else:\n",
    "                    cv2.putText(img, dominant_emotion, (x_min, y_min - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                img = display_EMO_PRED(img, (startX, startY, endX, endY), label, line_width=3)\n",
    "        \n",
    "        #отображаем зеркальное изображение в окне с заголовком Webcam\n",
    "        cv2.imshow('Webcam', cv2.flip(img, 1))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    #освобождаем ресурсы, связанные с камерой\n",
    "    cap.release()\n",
    "\n",
    "    #закрываем все окна OpenCV\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
